{"/application/applicationForm":{"title":"Application Form","data":{}},"/application":{"title":"Application Details","data":{"":"Potential participants to COLIEE-2024 should respond to this call for participation by submitting an application. Please send any questions to coliee_participation@nii.ac.jp.\nResearchers interested in using the COLIEE datasets outside of the scope of the Competition are allowed to do so. They still need to send the corresponding signed memorandums according to the instructions above, but not the application form. If you are a student, we request that the memorandum(s) is(are) signed by your supervisor.\nWe will send an acknowledgement to the email address supplied in the form once we have processed the form."}},"/application/statuteENMemorandumWaiver":{"title":"Statute EN Memorandum Waiver Form","data":{}},"/application/caseMemorandumWaiver":{"title":"Case Memorandum Waiver Form","data":{}},"/corpus":{"title":"Corpus Structure","data":{"":"The structure of the test corpora is derived from a general XML representation developed for use in RITEVAL, one of the tasks of the NII Testbeds and Community for Information access Research (NTCIR) project, as described at the following URL:http://sites.google.com/site/ntcir11riteval/The RITEVAL format was developed for the general sharing of information retrieval on a variety of domains."}},"/application/statuteJAMemorandumWaiver":{"title":"Statute JA Memorandum Waiver Form","data":{}},"/contact":{"title":"Contact","data":{"":"Contact information and FAQs about COLIEE 2024.For any inquiries, please contact us at: coliee_participation@nii.ac.jp"}},"/corpus/task1":{"title":"Task 1","data":{"":"The corpus is given as a flat list of files containing all query and noticed cases, for both the training and test datasets. The training dataset is described in a json file containing a mapping between the query case and a list of noticed cases, as in the example below:\n{\r\n  \"000001.txt\": [\"000005.txt\", \"012101.txt\"],  \r\n  \"003423.txt\": [\"398421.txt\", \"012101.txt\", \"173651.txt\"],  \r\n  \"012831.txt\": [\"000001.txt\"],  \r\n  ...\r\n}\nThe above is an example of a golden labels file for Task 1 containing three query cases (or \"base cases\"). The first query case is the file \"000001.txt\", which has 2 noticed cases (\"000005.txt\" and \"012101.txt\"). The second query case is the file named \"003423.txt\", which has 3 noticed cases (whose file names are \"021.txt\" and \"105.txt\"). The third query case (\"012831.txt\") has only one noticed case: \"000001.txt\".The test dataset json file contains only the list of query cases, and the task consists in populating the list of noticed cases for each query case."}},"/results":{"title":"Task Winners","data":{"":"Results for the Statute Law tasks have been sent directly to participants."}},"/overview":{"title":"Overview","data":{"":"Welcome to the 11th Competition on Legal Information Extraction and Entailment (COLIEE-2024). This competition is run in association with the International Workshop on Juris-Informatics (JURISIN 2024).The COLIEE-2024 workshop will be held during JURISIN 2024 from May 28-31, 2024, at the ACT CITY Hamamatsu, Hamamatsu, Shizuoka, Japan.","latest-updates#Latest Updates":"Results for the case law tasks have been released!Download Call for Participation","registration-and-contact#Registration and Contact":"COLIEE registration is due by December 31, 2023. If you wish to use previous COLIEE data for a trial, please contact coliee_participation@nii.ac.jp.","competition-tasks#Competition Tasks":"COLIEE-2024 includes four tasks:Task 1: Legal Case RetrievalTask 2: Legal Case EntailmentTask 3: Statute Law RetrievalTask 4: Legal Textual Entailment","sponsors#Sponsors":"Alberta Machine Intelligence Institute (AMII)\r\nUniversity of Alberta\nNational Institute of Informatics (NII)\nJurisage\nIntellicon"}},"/corpus/task3and4":{"title":"Tasks 3 and 4","data":{"":"The format of the COLIEE competition corpora derived from an NTCIR representation of confirmed relationships between questions and the articles and cases relevant to answering the questions, as in the following example:\n<pair label=\"Y\" id=\"H18-1-2\">\t\t\r\n  <t1>\t\r\n    (Seller's Warranty in cases of Superficies or Other Rights)Article 566  \r\n    (1)In cases where the subject matter of the sale is encumbered with for the purpose of a superficies, an emphyteusis, an easement, a right of retention or a pledge, if the buyer does not know the same and cannot achieve the purpose of the contract on account thereof, the buyer may cancel the contract. In such cases, if the contract cannot be cancelled, the buyer may only demand compensation for damages.  \r\n    (2)The provisions of the preceding paragraph shall apply mutatis mutandis in cases where an easement that was referred to as being in existence for the benefit of immovable property that is the subject matter of a sale, does not exist, and in cases where a leasehold is registered with respect to the immovable property.  \r\n    (3)In the cases set forth in the preceding two paragraphs, the cancellation of the contract or claim for damages must be made within one year from the time when the buyer comes to know the facts.  \r\n    (Seller's Warranty in cases of Mortgage or Other Rights) Article 567  \r\n    (1)If the buyer loses his/her ownership of immovable property that is the object of a sale because of the exercise of an existing statutory lien or mortgage, the buyer may cancel the contract.  \r\n    (2)If the buyer preserves his/her ownership by incurring expenditure for costs, he/she may claim reimbursement of those costs from the seller.  \r\n    (3)In the cases set forth in the preceding two paragraphs, the buyer may claim compensation if he/she suffered loss.  \r\n  </t1>\t  \r\n  <t2>\t\r\n    There is a limitation period on pursuance of warranty if there is restriction due to superficies on the subject matter, but there is no restriction on pursuance of warranty if the seller's rights were revoked due to execution of the mortgage.  \r\n  </t2>\t \r\n</pair> \nThe above is an example where query id \"H18-1-2\" is confirmed to be answerable from article numbers 566 and 567 (relevant to Task 3). The pair label \"Y\" in this example means the answer for this query is \"Yes\", which is entailed from the relevant articles (relevant to Task 4 and Task 5).For the Tasks 3 and 4, the training data will be the same. The groups who participate only in the Task 3 can disregard the pair label."}},"/corpus/task2":{"title":"Task 2","data":{"":"The corpus structure for Task 2 is given below:\n{\r\n  \"001\": [\"013.txt\"],  \r\n  \"002\": [\"003.txt\", \"045.txt\"],  \r\n  ...  \r\n}\nThe above is an example of Task 2 training data containing 2 files. Each query case has a separate folder, which is named with the query case id. That folder contains a file named \"base_case.txt\", which contains the raw text of the query case (with a few fragments suppressed), a file named \"entailed_fragment.txt\", which contains a fragment from the query case that is entailed by one or more paragraphs of a referenced case, and a folder named \"paragraphs\". That folder contains the paragraphs of said referenced case, one paragraph per file, which are named 001.txt to [n].txt (n being the number of paragraphs in the referenced case). The expected answer for each case is given as a list of paragraphs in the mapping file.Given the sample above, the file structure for the corpus would be:\ntask2_training_corpus  \r\n+--- 001  \r\n+------- base_case.txt  \r\n+------- entailed_fragment.txt  \r\n+------- paragraphs  \r\n+----------- 001.txt  \r\n+----------- 002.txt  \r\n+----------- ...  \r\n+----------- 046.txt  \r\n+--- 002  \r\n+------- base_case.txt  \r\n+------- entailed_fragment.txt  \r\n+------- paragraphs  \r\n+----------- 001.txt  \r\n+----------- 002.txt  \r\n+----------- ...  \r\n+----------- 211.txt  \r\n+--- train_labels.json  \nFor the query case 001, there are 46 paragraphs in the referenced case (among which is the expected answer, 013.txt, as given in the golden labels JSON file shown before). For the query case 002, there are 211 paragraphs in the referenced case, among which are the two which entail the fragment of text for that case (003.txt and 045.txt, as given in the golden labels file). For the case whose id is \"001\", the expected answer is \"013.txt\", meaning the entailed fragment (ie, the decision) in that query can be entailed from the paragraph id 013 in the given noticed case. The decision in the query is not the final decision of the case. This is a decision for a part of the case, and a paragraph that supports this decision should be identified in the given noticed case. The test corpora will not include the JSON file mapping, and the task is to predict which paragraph(s) entail(s) the decision given by the entailed_fragment.txt file in each case."}},"/resources":{"title":"Resources","data":{"previous-coliee-editions#Previous COLIEE editions:":"COLIEE 2023\nSummary Paper | Complete ProceedingsCOLIEE 2022\nSummary PaperCOLIEE 2021\nSummary Paper | Complete ProceedingsCOLIEE 2020\nSummary PaperCOLIEE 2019\nSummary PaperCOLIEE 2018\nCase Law Tasks | Statute Law TasksCOLIEE 2017\nCOLIEE 2016\nCOLIEE 2015\nCOLIEE 2014"}},"/evaluation":{"title":"Measuring the Competition Results","data":{"case-law-competition-results-tasks-1-and-2#Case Law Competition Results (Tasks 1 and 2)":"For Tasks 1 and 2, the evaluation metrics will be precision, recall, and F-measure:Precision = (the number of correctly retrieved cases(paragraphs) for all queries)\n(the number of retrieved cases(paragraphs) for all queries)\nRecall = (the number of correctly retrieved cases(paragraphs) for all queries)\n(the number of relevant cases(paragraphs) for all queries)\nF-measure = (2 x Precision x Recall)\n(Precision + Recall)In the evaluation of Task 1 and Task 2, we simply use micro-average (evaluation measure is calculated using the results of all queries) rather than macro-average (evaluation measure is calculated for each query and then take average).","statute-law-competition-results-tasks-3-and-4#Statute Law Competition Results (Tasks 3 and 4)":"For Task 3, the evaluation metrics will be precision, recall, and F2-measure (since IR process is a pre-process to select candidate articles for providing candidates which will be used in the entailment process, we put emphasis on recall), and it is:Precision = average of (the number of correctly retrieved articles for each query)\n(the number of retrieved articles for each query)\nRecall = average of (the number of correctly retrieved articles for each query)\n(the number of relevant articles for each query)\nF2-measure = (5 x Precision x Recall)\n(4 x Precision + Recall)In addition to the above evaluation measures, ordinal information retrieval measures such as Mean Average Precision and R-precision can be used for discussing the characteristics of the submission results.In COLIEE 2024, the method used to calculate the final evaluation score of all queries is macro-average (evaluation measure is calculated for each query and their average is used as the final evaluation measure) instead of micro-average (evaluation measure is calculated using results of all queries).For Task 4, the evaluation measure will be accuracy, with respect to whether the yes/no question was correctly confirmed:Accuracy = (the number of queries which were correctly confirmed as true or false)\r\n(the number of all queries)"}},"/tasks":{"title":"Task Details","data":{"important-notice#Important Notice":"Participants should clearly mention what dataset was used (for example: pretrained by Wikipedia dump data as of 2022xxxx, fine-tuned by...) for reproducibility purposes. Participants can use any external data, but it is assumed that they do not use the test dataset and/or something which could directly contain the correct answers of the test dataset."}},"/schedule":{"title":"Schedule","data":{"":"Nov 01, 2023: Training data releaseDec 13, 2023: Test data releaseDec 13, 2023: Registration deadline for COLIEEJan 04, 2024: Submission deadline of Task 3 test runsJan 13, 2024: Return of competition test run rankings/assessments for task 3 and announcements of answers (relevant article(s) for each question) for task 4Jan 18, 2024: Submission deadline for Tasks 1, 2, and 4Jan 20, 2024: Announcements of rankings/assessments for Tasks 1, 2, 4Jan 31, 2024: Paper submission deadline for the COLIEE workshopFeb 29, 2024: Notification of acceptance for the COLIEE workshop paperMar 25, 2024: Camera-ready copy deadlineMay 28-31, 2024: 2024 JURISIN workshop (one day for the COLIEE workshop - TBD)Submission deadline: 23:59 AoE for all dates above."}},"/submission":{"title":"Submission Guidelines","data":{"":"Participants are required to submit a paper on their method and experimental results. At least one of the authors of an accepted paper has to present the paper at the COLIEE session of JURISIN 2024. Papers should conform to the standards set out at the JURISIN 2024 webpage:JURISIN 2024\nSubmission page at JURISIN 2024We will publish COLIEE papers in one of two categories according to the quality of the papers:\nProceedings in LNAI series by Springer Verlag which will (electronically) be distributed at the COLIEE workshop\nLocal Proceedings published by IsAI-JSAI (parent symposium of JURISIN 2024) for papers which are not qualified for LNAI publication, but are selected to be presented at the COLIEE workshop.\nParticipants should clearly mention what dataset was used (for example: pretrained by Wikipedia dump data as of 2022xxxx, fine-tuned by...) for reproducibility purposes. Participants can use any external data, but it is assumed that they do not use the test dataset and/or something which could directly contain the correct answers of the test dataset.","results-submission-format#Results Submission Format":"","task-1#Task 1":"For Task 1, a submission should consist of a single ASCII text file. Use a single space to separate columns, with three columns per line as follows:\n000001 000018 univABC  \r\n000001 000045 univABC  \r\n000001 000130 univABC  \r\n000002 000433 univABC  \r\n...\nwhere:\nThe first column is the query file name.\nThe second column is the official case number of the retrieved case.\nThe third column is called the \"run tag\" and should be a unique identifier for the submitting group, i.e., each run should have a different tag that identifies the group. Please restrict run tags to 12 or fewer letters and numbers, with no punctuation.","task-2#Task 2":"For Task 2, a submission should consist of a single ASCII text file. Use a single space to separate columns, with three columns per line as follows:\n001 013 univABC  \r\n002 037 univABC  \r\n002 002 univABC  \r\n003 008 univABC  \r\n...  \nwhere:\nThe first column is the query id.\nThe second column is the paragraph number which entails the decision.\nThe third column is called the \"run tag\" and should be a unique identifier for the submitting group, i.e., each run should have a different tag that identifies the group. Please restrict run tags to 12 or fewer letters and numbers, with no punctuation.","task-3#Task 3":"Submission format in Task 3 is the TREC eval format used in trec_eval program. Use a single space to separate columns, with six columns per line as follows:H21-5-3 Q0 213 1 0.8 univABCWhere\nThe first column is the query id.\nThe second column is \"iter\" for trec_eval and not used in the evaluation. Information of the column will be ignored. But please write Q0 in this column.\nThe third column is the official article number of the retrieved article.\nThe fourth column is the rank of the retrieved articles.\nThe fifth column is the similarity value (float value) of the retrieved articles.\nThe sixth column is called the \"run tag\" and should be a unique identifier for the submitting group, i.e., each run should have a different tag that identifies the group. Please restrict run tags to 12 or fewer letters and numbers, with no punctuation.","task-4#Task 4":"For Task 4, again a submission should consist of a single ASCII text file. Use a single space to separate columns, with three columns per line as follows:\nH18-1-2 Y univABC  \r\nH18-5-A N univABC  \r\nH19-19-I Y univABC  \r\nH21-5-3 N univABC  \r\n...  \nwhere:\nThe first column is the query id.\n\"Y\" or \"N\" indicating whether the Y/N question was confirmed to be true (\"Y\") by the relevant articles, or confirmed to be false (\"N\").\nThe third column is called the \"run tag\" and should be a unique identifier for the submitting group, i.e., each run should have a different tag that identifies the group. Please restrict run tags to 12 or fewer letters and numbers, with no punctuation."}},"/results/task1":{"title":"Task 1 Results","data":{"":"Team\tFile\tF1\tPrecision\tRecall\tTQM\ttask1_test_answer_2024_run1.txt\t0.4432\t0.5057\t0.3944\tTQM\ttask1_test_answer_2024_run3.txt\t0.4342\t0.5082\t0.3790\tUMNLP\ttask1_umnlp_run1.txt\t0.4134\t0.4000\t0.4277\tUMNLP\ttask1_umnlp_run2.txt\t0.4097\t0.3755\t0.4507\tUMNLP\ttask1_umnlp_runs_combined.txt\t0.4046\t0.3597\t0.4622\tYR\ttask1_yr_run1.json\t0.3605\t0.3210\t0.4110\tTQM\ttask1_test_answer_2024_run2.txt\t0.3548\t0.4196\t0.3073\tYR\ttask1_yr_run2.json\t0.3483\t0.3245\t0.3758\tYR\ttask1_yr_run3.json\t0.3417\t0.3184\t0.3688\tJNLP\t64b7b-07f39.txt\t0.3246\t0.3110\t0.3393\tJNLP\t07f39.txt\t0.3222\t0.3347\t0.3105\tJNLP\t64b7b-48fe5.txt\t0.3103\t0.3017\t0.3195\tWJY\tsubmit_1.txt\t0.3032\t0.2700\t0.3457\tBM24\ttask1_test_result.txt\t0.1878\t0.1495\t0.2522\tCAPTAIN\tcaptain_mstr.txt\t0.1688\t0.1793\t0.1594\tCAPTAIN\tcaptain_ft5.txt\t0.1574\t0.1586\t0.1562\tNOWJ\tnowjtask1run2.txt\t0.1313\t0.0895\t0.2465\tNOWJ\tnowjtask1run3.txt\t0.1306\t0.0957\t0.2055\tNOWJ\tnowjtask1run1.txt\t0.1224\t0.0813\t0.2478\tWJY\tsubmit_3.txt\t0.1179\t0.0870\t0.1831\tWJY\tsubmit_2.txt\t0.1174\t0.0824\t0.2042\tMIG\ttest1_ans.json\t0.0508\t0.0516\t0.0499\tUBCS\trun3.txt\t0.0276\t0.0140\t0.7196\tUBCS\trun2.txt\t0.0275\t0.0140\t0.7177\tUBCS\trun1.txt\t0.0272\t0.0139\t0.7100\tCAPTAIN\tcaptain_bm25.txt\t0.0019\t0.0019\t0.0019\t\tTotal Teams:\t10\t\tTotal submissions:\t26"}},"/results/task2":{"title":"Task 2 Results","data":{"":"Team\tFile\tF1\tPrecision\tRecall\tAMHR\tamhr.mt53bk2r.txt\t0.6512\t0.6364\t0.6667\tCAPTAIN\tcaptainfs2.txt\t0.6360\t0.7281\t0.5646\tJNLP\t07f39.txt\t0.6320\t0.6967\t0.5782\tCAPTAIN\tcaptainzs2.txt\t0.6235\t0.7700\t0.5238\tCAPTAIN\tcaptainzs3.txt\t0.6235\t0.7700\t0.5238\tNOWJ\tt5.txt\t0.6117\t0.6181\t0.6054\tJNLP\tjoin-constr.txt\t0.6045\t0.6694\t0.5510\tOVGU\t2ovgurun1.txt\t0.5962\t0.5636\t0.6327\tNOWJ\tweak.txt\t0.5946\t0.5906\t0.5986\tJNLP\tjoin.txt\t0.5912\t0.6378\t0.5510\tOVGU\t2ovgurun2.txt\t0.5705\t0.5506\t0.5918\tOVGU\t2ovgurun3.txt\t0.5532\t0.5000\t0.6190\tNOWJ\tbert.txt\t0.5197\t0.5032\t0.5374\tMIG\ttask2.mig1.json\t0.4701\t0.5673\t0.4014\tMIG\ttask2.mig2.json\t0.4696\t0.5800\t0.3946\tAMHR\tamhr.lsbk2m42.txt\t0.3542\t0.3617\t0.3469\tAMHR\tamhr.lsbk1.txt\t0.3320\t0.4100\t0.2789\tMIG\ttask2.mig3.json\t0.1364\t0.0979\t0.2245\t\tTotal Teams:\t6\t\tTotal submissions:\t18"}},"/tasks/task1":{"title":"Task 1","data":{"legal-case-retrieval#Legal Case Retrieval":"Our goal is to explore and evaluate legal document retrieval technologies that are both effective and reliable. The task investigates the performance of systems that search a set of case laws that support the unseen case law. The goal of the task is to return 'noticed cases' in the given collection to a query. We call a case is 'noticed' to a query if the case is referenced by the query case. In this task, the references are redacted from the query case contents, because our goal is to measure how accurately a machine can capture decision-supporting cases for a given case.A corpus composed of Federal Court of Canada case laws will be provided. The process of executing the new query cases over the existing cases and generating the experimental runs should be entirely automatic. All query and noticed cases will be provided as a pool. In the training data, we will also disclose which are the noticed cases for each query case. In the test data, only the query cases will be given and the task is to predict which cases should be noticed with respect to each of the test query cases.There should be no human intervention at any stage, including modifications to your retrieval system motivated by an inspection of the test queries. You won't have access to the test labels before you submit your runs.At most three runs from each group will be assessed. The submission format and evaluation methods are described below."}},"/tasks/task2":{"title":"Task 2","data":{"legal-case-entailment#Legal Case Entailment":"Our goal is to predict the decision of a new case by entailment from previous relevant cases. As a simpler version of predicting a decision, a decision of a new case and a noticed case will be given as a query. Then, your legal textual entailment system identifies which paragraph in the noticed case entails the decision, by comparing the meanings between queries and the paragraphs.The task investigates the performance of systems that identifies a paragraph that entails the decision of an unseen case. Training data consists of triples of a query, a noticed case, and a paragraph number of the noticed case by which the decision of the query is entailed. The process of executing the queries over the noticed cases and generating the experimental runs should be entirely automatic. Test data will include only queries and noticed cases, but no paragraph numbers.There should be no human intervention at any stage, including modifications to your retrieval system motivated by an inspection of the test queries. 'Decision', in this context, does not mean the final decision of a case, but rather a conclusion expressed by the judge which is entailed by one or more particular paragraphs from the noticed case. In our dataset, this information is packaged in a file named 'entailed_fragment.txt'."}},"/tasks/task3":{"title":"Task 3","data":{"statute-law-retrieval#Statute Law Retrieval":"Our goal is to explore and evaluate legal document retrieval technologies that are both effective and reliable. The task investigates the performance of systems that search a static set of civil law articles using previously unseen queries. The goal of the task is to return relevant articles in the collection to a query. We call an article as \"Relevant\" to a query if the query sentence can be answered Yes/No, entailed from the meaning of the article.If combining the meanings of more than one article (e.g., \"A\", \"B\", and \"C\") can answer a query sentence, then all the articles (\"A\", \"B\", and \"C\") are considered \"Relevant\". If a query can be answered by an article \"D\", and it can be also answered by another article \"E\" independently, we also consider both of the articles \"D\" and \"E\" are \"Relevant\". This task requires the retrieval of all the articles that are relevant to answering a query.Japanese civil law articles (English translation besides Japanese) will be provided, and training data consists of pairs of a query and relevant articles. The process of executing the queries over the articles and generating the experimental runs should be entirely automatic. Test data will include only queries but no relevant articles.There should be no human intervention at any stage, including modifications to your retrieval system motivated by an inspection of the queries. You should not materially modify your retrieval system between the time you downloaded the queries and the time you submit your runs.At most three runs from each group will be assessed. The submission format and evaluation methods are described below."}},"/tasks/task4":{"title":"Task 4","data":{"legal-textual-entailment#Legal Textual Entailment":"Our goal is to construct Yes/No question answering systems for legal queries, by entailment from the relevant articles. If a 'Yes/No' legal bar exam question is given, your legal information retrieval system retrieves relevant Civil Law articles. Then, the task investigates the performance of systems that answer 'Yes' or 'No' to previously unseen queries by comparing the meanings between queries and your retrieved Civil Law articles. Training data consists of triples of a query, relevant article(s), a correct answer \"Y\" or \"N\". Test data will include only queries and relevant articles, but no 'Y/N' label.There should be no human intervention at any stage, including modifications to your retrieval system motivated by an inspection of the queries. You should not materially modify your retrieval system between the time you downloaded the queries and the time you submit your runs.At most three runs for each group should be assessed. The submission format and evaluation methods are described below."}}}